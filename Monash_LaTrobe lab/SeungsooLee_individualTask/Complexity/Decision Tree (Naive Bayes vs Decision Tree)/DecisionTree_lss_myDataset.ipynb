{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLSajBTl4xzP"
   },
   "source": [
    "# FIT5202 Data processing for big data\n",
    "\n",
    "##  Activity: Machine Learning with Spark (Classification Using Decision Tree and Random Forest)\n",
    "\n",
    "Last week we learnt about basics of machine learning with Apache Spark. **``MLlib``** is Apache Spark's scalable machine learning library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "- ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- Persistence: saving and load algorithms, models, and Pipelines\n",
    "- Utilities: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "We looked into transformers, estimators and machine learning pipeline in the last weeks tutorial activity.\n",
    "\n",
    "This week we have learnt about decision tree and random forest algorithms in the lecture. We will look into how to use the two different popular family of classification and regression methods; Decision Trees and Random forests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCtgNkcn5al0"
   },
   "source": [
    "# Decision Tree\n",
    "\n",
    "A *decision tree* is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.\n",
    "\n",
    "In this exercise, we will be using Apache spark to create a decision tree. Basically the dataset (shown below) lists the conditions which impacts if a game of tennis can be played outside or not. The values of outlook, temperature, humidity and wind are described and outcome that the game was played or not under these conditions. \n",
    "\n",
    "![Dataset for Decision Tree](https://camo.githubusercontent.com/750443a0828b170b12a3eeaf42b7c1aa5e7c25b8/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3630302f312a426e3364345a3632736f66334b3455315f3070536c512e6a706567)\n",
    "\n",
    "We will build the decision tree like the one below.\n",
    "![Decision Tree](https://camo.githubusercontent.com/210366841be13b6b5ff9fa3e4e8e7819679c5ad4/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a546c547a677438495f35645553624d5a6d524b7971512e6a706567)\n",
    "\n",
    "We will go through the code and explain which part of the code is doing what in the codebase.\n",
    "\n",
    "## 1. Include the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fi2vW576DT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in ./jupyter/lib/python3.6/site-packages (2.4)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./jupyter/lib/python3.6/site-packages (from networkx) (4.4.1)\r\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "# Uncomment the following line to install networkx\n",
    "!pip install networkx\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fZMR7Bq3Orj"
   },
   "source": [
    "## 2.  Instantiate the spark context\n",
    "\n",
    "We will use and import **`SparkContext`** from **`pyspark`**, which is the main entry point for Spark Core functionality. The **`SparkSession`** object provides methods used to create DataFrames from various input sources. \n",
    "A [DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) is equivalent to a relational table in Spark SQL, and can be created using various functions in SparkSession. Once created, it can be manipulated using the various domain-specific-language (DSL) functions defined in: [DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame), [Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0Yzy1Vf3Qhx"
   },
   "outputs": [],
   "source": [
    "sc.stop()#to not multiple SparkContext\n",
    "sc = SparkContext(master=\"local[*]\", appName=\"Decision Tree\")\n",
    "sqlContext = SQLContext(sc)\n",
    "attr_name_info_gain = {}\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-uo1Yk835-K"
   },
   "source": [
    "## 3.  Declare schema of the dataset. \n",
    "We have created 2 variables for storing the schema and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hwXu-aSS4Crr"
   },
   "outputs": [],
   "source": [
    "attrs = [\"outlook\",\"temp\",\"humidity\",\"wind\"]\n",
    "attrs_type = {\"outlook\":\"string\",\"temp\":\"string\",\"humidity\":\"string\",\"wind\":\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5cv3TpX4GaX"
   },
   "source": [
    "## 4.  Calculate the Information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssgzivUi4JOv"
   },
   "outputs": [],
   "source": [
    "def calculate_info_gain(entropy, joined_df, total_elements):\n",
    "    attr_entropy = 0.0\n",
    "    for anAttributeData in joined_df.rdd.collect():\n",
    "        yes_class_count = anAttributeData[1]\n",
    "        no_class_count = anAttributeData[2]\n",
    "        if yes_class_count is None:\n",
    "            yes_class_count = 0\n",
    "        elif no_class_count is None:\n",
    "            no_class_count = 0\n",
    "\n",
    "        count_of_class = yes_class_count + no_class_count\n",
    "        classmap = {'y' : yes_class_count, 'n' : no_class_count}\n",
    "        attr_entropy = attr_entropy + ((count_of_class / total_elements) *\\\n",
    "                                       calculate_entropy(count_of_class, classmap))\n",
    "\n",
    "    gain = entropy - attr_entropy\n",
    "\n",
    "    return gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLzjfSrb4PDf"
   },
   "source": [
    "## 5.  Attribute information gain data preparation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mylu_NWk4UCe"
   },
   "outputs": [],
   "source": [
    "def get_attr_info_gain_data_prep(attr_name, data, entropy, total_elements, where_condition):\n",
    "\n",
    "    if not where_condition:\n",
    "        attr_grp_y = data.where(col('y') == 'yes').groupBy(attr_name).agg({\"y\": 'count'})\\\n",
    "            .withColumnRenamed('count(y)','played_count')\n",
    "    else:\n",
    "        attr_grp_y = data.where(\" y like '%yes%'  \" + where_condition).groupBy(attr_name).agg({\"y\": 'count'})\\\n",
    "            .withColumnRenamed('count(y)','played_count')\n",
    "\n",
    "    if not where_condition:\n",
    "        attr_grp_n = data.where(col('y') == 'no').groupBy(attr_name).agg({\"y\": 'count'})\\\n",
    "            .withColumnRenamed(attr_name,'n_' + attr_name)\\\n",
    "            .withColumnRenamed('count(y)','not_played_count')\n",
    "    else:\n",
    "        attr_grp_n = data.where(\" y like '%no%'  \" + where_condition).groupBy(attr_name).agg({\"y\": 'count'})\\\n",
    "            .withColumnRenamed(attr_name,'n_' + attr_name)\\\n",
    "            .withColumnRenamed('count(y)','not_played_count')\n",
    "\n",
    "    joined_df = attr_grp_y.join(attr_grp_n, on = [col(attr_grp_y.columns[0]) == col(attr_grp_n.columns[0])], how='outer' )\\\n",
    "        .withColumn(\"total\", col(attr_grp_y.columns[0]) + col(attr_grp_n.columns[0]))\\\n",
    "        .select(attr_grp_y.columns[0], attr_grp_y.columns[1],\\\n",
    "                 attr_grp_n.columns[1]) \\\n",
    "\n",
    "    gain_for_attribute = calculate_info_gain(entropy, joined_df, total_elements)\n",
    "    attr_name_info_gain[attr_name] = gain_for_attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2J9BdMa4dNf"
   },
   "source": [
    "## 6.  Calculate the entropy of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihpWpWoB4f4m"
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(total_elements, elements_in_each_class):\n",
    "    # for target set S having 2 class 0 and 1, the entropy is -p0logp0 -p1logp1\n",
    "    # here the log is of base 2\n",
    "    # elements_in_each_class is a dictionary where the key is class label and the\n",
    "    # value is number of elements in that class\n",
    "    keysInMap = list(elements_in_each_class.keys())\n",
    "    entropy = 0.0\n",
    "\n",
    "    for aKey in keysInMap:\n",
    "        number_of_elements_in_class = elements_in_each_class.get(aKey)\n",
    "        if number_of_elements_in_class == 0:\n",
    "            continue\n",
    "        ratio = number_of_elements_in_class/total_elements\n",
    "        entropy = entropy - ratio * np.log2(ratio)\n",
    "\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBct4zUv4lAH"
   },
   "source": [
    "## 7.  Process the data\n",
    "\n",
    "As we build the tree, we will need to get data corresponding to that branch of the tree only. The ‘where_condition’ attribute will contain these predicates.\n",
    "\n",
    "We group the records in the file which have outcome as ‘yes’ for the attribute names passed\n",
    "\n",
    "- For first time, the where_condition will be blank,\n",
    "- Second iteration onwards, after root of the tree is found, we will have **where_condition**\n",
    "- **excludedAtttts** will contain the list of attributes which are already processed so that we dont need to process again.\n",
    "- **data** is the spark dataframe for this file\n",
    "- **played** — count when match was played\n",
    "- **notplayed** — count when match was not played\n",
    "- **Where_condition** — condition used to select the data, as and when attributes are processed we will keep chaging this condition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6hFGdrY5Kc9"
   },
   "outputs": [],
   "source": [
    "def process_dataset(excludedAttrs, data, played, notplayed, where_condition):\n",
    "    total_elements = played + notplayed\n",
    "    subs_info = {\"played\" : played, \"notplayed\" : notplayed}\n",
    "    entropy = calculate_entropy(total_elements, subs_info)\n",
    "    print (\"entropy is \" + str(entropy))\n",
    "    global attr_name_info_gain\n",
    "    attr_name_info_gain = dict()\n",
    "\n",
    "    for attr in attrs:\n",
    "        if attr not in excludedAttrs:\n",
    "            get_attr_info_gain_data_prep(attr, data, entropy, total_elements, where_condition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jA9wCJWX5Pqu"
   },
   "source": [
    "## 8. Build the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_sy7jPx5UN-"
   },
   "outputs": [],
   "source": [
    "def build_tree(max_gain_attr, processed_attrs, data, where_condition):\n",
    "    attrValues = sqlContext.sql(\"select distinct \" + max_gain_attr + \" from data  where 1==1 \" + where_condition)\n",
    "    orig_where_condition = where_condition\n",
    "\n",
    "    for aValueForMaxGainAttr in attrValues.rdd.collect():\n",
    "        adistinct_value_for_attr = aValueForMaxGainAttr[0]\n",
    "        G.add_edges_from([(max_gain_attr, adistinct_value_for_attr)])\n",
    "\n",
    "        if attrs_type[max_gain_attr] == \"string\":\n",
    "            where_condition = str(orig_where_condition + \" and \" + max_gain_attr + \"=='\" + adistinct_value_for_attr + \"'\")\n",
    "        else:\n",
    "            where_condition = str(orig_where_condition + \" and \" + max_gain_attr + \"==\" + adistinct_value_for_attr)\n",
    "\n",
    "        played_for_attr = sqlContext.sql(\"select * from data where y like '%yes%' \" + where_condition).count()\n",
    "        notplayed_for_attr = sqlContext.sql(\"select * from data where y like '%no%' \" + where_condition).count()\n",
    "        # if either has zero value then entropy for this attr will be zero and its the last attr in the tree\n",
    "        leaf_values = []\n",
    "        if played_for_attr == 0 or notplayed_for_attr == 0:\n",
    "            leaf_node = sqlContext.sql(\"select distinct y from data where 1==1 \" + where_condition)\n",
    "            for leaf_node_data in leaf_node.rdd.collect():\n",
    "                G.add_edges_from([(adistinct_value_for_attr, str(leaf_node_data[0]))])\n",
    "            continue\n",
    "        process_dataset(processed_attrs, data, played_for_attr, notplayed_for_attr, where_condition)\n",
    "        if not attr_name_info_gain: # we processed all attributes\n",
    "            # attach leaf node\n",
    "            leaf_node = sqlContext.sql(\"select distinct y from data where 1==1 \" + where_condition)\n",
    "            for leaf_node_data in leaf_node.rdd.collect():\n",
    "                G.add_edges_from([(adistinct_value_for_attr, str(leaf_node_data[0]))])\n",
    "            continue # we are done for this branch of tree\n",
    "\n",
    "        # get the attr with max info gain under aValueForMaxGainAttr\n",
    "        # sort by info gain\n",
    "        sorted_by_info_gain = sorted(attr_name_info_gain.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        new_max_gain_attr = sorted_by_info_gain[0][0]\n",
    "        if sorted_by_info_gain[0][1] == 0:\n",
    "            # under this where condition, records dont have entropy\n",
    "            leaf_node = sqlContext.sql(\"select distinct y from data where 1==1 \" + where_condition)\n",
    "            # there might be more than one leaf node\n",
    "            for leaf_node_data in leaf_node.rdd.collect():\n",
    "                G.add_edges_from([(adistinct_value_for_attr, str(leaf_node_data[0]))])\n",
    "            continue # we are done for this branch of tree\n",
    "\n",
    "        G.add_edges_from([(adistinct_value_for_attr, new_max_gain_attr)])\n",
    "        processed_attrs.append(new_max_gain_attr)\n",
    "        build_tree(new_max_gain_attr, processed_attrs, data, where_condition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOLRIW6U5Z7m"
   },
   "source": [
    "## 9. Load the dataset and draw the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X80hfisT5jgG",
    "outputId": "5f8369df-277e-463b-888b-c7d576febaca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy is 0.9612366047228759\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`outlook`' given input columns: [Gender, y, Lover, GPA, Laptop];;\\n'Aggregate ['outlook], ['outlook, count(y#1815) AS count(y)#1859L]\\n+- Filter (y#1815 = yes)\\n   +- Relation[Gender#1811,Lover#1812,GPA#1813,Laptop#1814,y#1815] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/jupyter/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1261.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`outlook`' given input columns: [Gender, y, Lover, GPA, Laptop];;\n'Aggregate ['outlook], ['outlook, count(y#1815) AS count(y)#1859L]\n+- Filter (y#1815 = yes)\n   +- Relation[Gender#1811,Lover#1812,GPA#1813,Laptop#1814,y#1815] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:169)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:188)\n\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f8d6ac3536ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from data WHERE y like  '%y%' \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnotplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from data WHERE y like  '%n%' \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotplayed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# sort by info gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msorted_by_info_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_name_info_gain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-61fd4c4d35b3>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(excludedAttrs, data, played, notplayed, where_condition)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexcludedAttrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mget_attr_info_gain_data_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-9f7b50460c5e>\u001b[0m in \u001b[0;36mget_attr_info_gain_data_prep\u001b[0;34m(attr_name, data, entropy, total_elements, where_condition)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwhere_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mattr_grp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count(y)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'played_count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/lib/python3.6/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exprs should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`outlook`' given input columns: [Gender, y, Lover, GPA, Laptop];;\\n'Aggregate ['outlook], ['outlook, count(y#1815) AS count(y)#1859L]\\n+- Filter (y#1815 = yes)\\n   +- Relation[Gender#1811,Lover#1812,GPA#1813,Laptop#1814,y#1815] csv\\n\""
     ]
    }
   ],
   "source": [
    "data = sqlContext.read.format('com.databricks.spark.csv').option('header', 'true')\\\n",
    "        .option('delimiter', ';').load(\"lss_myDataset.txt\")\n",
    "\n",
    "data.registerTempTable('data')\n",
    "played = sqlContext.sql(\"select * from data WHERE y like  '%y%' \").count()\n",
    "notplayed = sqlContext.sql(\"select * from data WHERE y like  '%n%' \").count()\n",
    "process_dataset([], data, played, notplayed, '')\n",
    "# sort by info gain\n",
    "sorted_by_info_gain = sorted(attr_name_info_gain.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "processed_attrs = []\n",
    "max_gain_attr = sorted_by_info_gain[0][0]\n",
    "processed_attrs.append(max_gain_attr)\n",
    "build_tree(max_gain_attr, processed_attrs, data, '')\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Graph.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2a3cx1eoNk15"
   },
   "source": [
    "## 10. Stoppping the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N82jpHt5Nk17"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufnjuRA38fpK"
   },
   "source": [
    "## Congratulations on finishing this activity. See you next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5202 - Classification Using Decision Tree and Random Forest.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
