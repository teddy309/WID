{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access using SparkSQL and Dataframe\n",
    "\n",
    "## Activity : Projection and Selection Queries\n",
    "\n",
    "In this module, you will practice how to write codes to retrieve data using Spark SQL and Dataframes API.\n",
    "\n",
    "The complete list of Dataframe functions can be accessed from [here](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html), [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) and [here](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.functions$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we will use HR schema as shown below\n",
    "![hr](HR.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION\n",
    "The first section of this scipt is the initialization section. \n",
    "In this section, we are preparing Spark environment to recognize and process SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#additional \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "# In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). \n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as `local[k]`.\n",
    "# The `appName` field is a name to be shown on the Sparking cluster UI. \n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[3]\", appName=\"Introduction to Apache Spark\")\n",
    "spark = SparkSession(sparkContext=sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA STRUCTURE DEFINITION\n",
    "\n",
    "In this section, we are preparing the data structure to match the datafiles provided as the datasources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES TABLE\n",
    "scCountries = StructType([StructField(\"country_id\",StringType()),StructField(\"country_name\",StringType()),StructField(\"region_id\",IntegerType())])\n",
    "\n",
    "#DEPARTMENTS TABLE\n",
    "scDepartments = StructType([StructField(\"department_id\",IntegerType()),\n",
    "StructField(\"department_name\",StringType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"location_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#EMPLOYEES TABLE\n",
    "scEmployees = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"first_name\",StringType()),\n",
    "StructField(\"last_name\",StringType()),\n",
    "StructField(\"email\",StringType()),\n",
    "StructField(\"phone_number\",StringType()),\n",
    "StructField(\"hire_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"salary\",IntegerType()),\n",
    "StructField(\"commission_pct\",FloatType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOBS TABLE\n",
    "scJobs = StructType([\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"job_title\",StringType()),\n",
    "StructField(\"min_salary\",IntegerType()),\n",
    "StructField(\"max_salary\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOB_HISTORY TABLE\n",
    "scJob_history = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"start_date\",StringType()),\n",
    "StructField(\"end_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#LOCATIONS TABLE\n",
    "scLocations = StructType([\n",
    "StructField(\"location_id\",IntegerType()),\n",
    "StructField(\"street_address\",StringType()),\n",
    "StructField(\"postal_code\",StringType()),\n",
    "StructField(\"city\",StringType()),\n",
    "StructField(\"state_province\",StringType()),\n",
    "StructField(\"country_id\",StringType())\n",
    "])\n",
    "\n",
    "#REGIONS TABLE\n",
    "scRegions = StructType([\n",
    "StructField(\"region_id\",IntegerType()),\n",
    "StructField(\"region_name\",StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES DATA\n",
    "dataCountries = sc.textFile('COUNTRIES.csv')\n",
    "dataCountries = dataCountries.map(lambda x: x.split(','))\n",
    "dataCountries = dataCountries.map(lambda x: [x[0],x[1], int(x[2])])\n",
    "\n",
    "#DEPARTMENTS DATA\n",
    "dataDepartments = sc.textFile('DEPARTMENTS.csv')\n",
    "dataDepartments = dataDepartments.map(lambda x: x.split(','))\n",
    "dataDepartments = dataDepartments.map(lambda x: [int(x[0]),x[1], int(x[2]), int(x[3])])\n",
    "\n",
    "#EMPLOYEES DATA\n",
    "dataEmployees = sc.textFile('EMPLOYEES.csv')\n",
    "dataEmployees = dataEmployees.map(lambda x: x.split(','))\n",
    "dataEmployees = dataEmployees.map(lambda x: [int(x[0]),x[1], x[2], \\\n",
    "                                             x[3],x[4], x[5], x[6], \\\n",
    "                                             int(x[7]),float(x[8]), int(x[9]), int(x[10])\\\n",
    "                                            ])\n",
    "\n",
    "#JOBS_DATA\n",
    "dataJobs = sc.textFile('JOBS.csv')\n",
    "dataJobs = dataJobs.map(lambda x: x.split(','))\n",
    "dataJobs = dataJobs.map(lambda x: [x[0],x[1], \\\n",
    "                                   int(x[2]),int(x[3])\\\n",
    "                                   ])\n",
    "\n",
    "#JOB_HISTORY_DATA\n",
    "dataJob_history = sc.textFile('JOB_HISTORY.csv')\n",
    "dataJob_history = dataJob_history.map(lambda x: x.split(','))\n",
    "dataJob_history = dataJob_history.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],int(x[4])\\\n",
    "                                   ])\n",
    "\n",
    "#LOCATION_DATA\n",
    "dataLocations = sc.textFile('LOCATIONS.csv')\n",
    "dataLocations = dataLocations.map(lambda x: x.split(','))\n",
    "dataLocations = dataLocations.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],x[4],x[5]\\\n",
    "                                   ])\n",
    "#REGIONS DATA\n",
    "dataRegions = sc.textFile('REGIONS.csv')\n",
    "dataRegions = dataRegions.map(lambda x: x.split(','))\n",
    "dataRegions = dataRegions.map(lambda x: [int(x[0]),x[1] ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCountries = spark.createDataFrame(dataCountries,schema=scCountries) \n",
    "dfCountries.createOrReplaceTempView(\"dataCountries\")\n",
    "\n",
    "dfDepartments = spark.createDataFrame(dataDepartments,schema=scDepartments) \n",
    "dfDepartments.createOrReplaceTempView(\"dataDepartments\")\n",
    "\n",
    "dfEmployees = spark.createDataFrame(dataEmployees,schema=scEmployees) \n",
    "dfEmployees.createOrReplaceTempView(\"dataEmployees\")\n",
    "\n",
    "dfJobs = spark.createDataFrame(dataJobs,schema=scJobs) \n",
    "dfJobs.createOrReplaceTempView(\"dataJobs\")\n",
    "\n",
    "dfJob_history = spark.createDataFrame(dataJob_history,schema=scJob_history) \n",
    "dfJob_history.createOrReplaceTempView(\"dataJob_history\")\n",
    "\n",
    "dfLocations = spark.createDataFrame(dataLocations,schema=scLocations) \n",
    "dfLocations.createOrReplaceTempView(\"dataLocations\")\n",
    "\n",
    "dfRegions = spark.createDataFrame(dataRegions,schema=scRegions) \n",
    "dfRegions.createOrReplaceTempView(\"dataRegions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROJECTION QUERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Display employee's first name with their last name, along with their job id and their salary.\n",
    "\n",
    "Ensure you have the same format as expected output below \n",
    "\n",
    "![picture](lab2_project0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------+\n",
      "|         FullName|    job_id|salary|\n",
      "+-----------------+----------+------+\n",
      "|      Steven King|   AD_PRES| 24000|\n",
      "|    Neena Kochhar|     AD_VP| 17000|\n",
      "|      Lex De Haan|     AD_VP| 17000|\n",
      "| Alexander Hunold|   IT_PROG|  9000|\n",
      "|      Bruce Ernst|   IT_PROG|  6000|\n",
      "|     David Austin|   IT_PROG|  4800|\n",
      "|  Valli Pataballa|   IT_PROG|  4800|\n",
      "|    Diana Lorentz|   IT_PROG|  4200|\n",
      "|  Nancy Greenberg|    FI_MGR| 12008|\n",
      "|    Daniel Faviet|FI_ACCOUNT|  9000|\n",
      "|        John Chen|FI_ACCOUNT|  8200|\n",
      "|   Ismael Sciarra|FI_ACCOUNT|  7700|\n",
      "|Jose Manuel Urman|FI_ACCOUNT|  7800|\n",
      "|        Luis Popp|FI_ACCOUNT|  6900|\n",
      "|     Den Raphaely|    PU_MAN| 11000|\n",
      "|   Alexander Khoo|  PU_CLERK|  3100|\n",
      "|     Shelli Baida|  PU_CLERK|  2900|\n",
      "|     Sigal Tobias|  PU_CLERK|  2800|\n",
      "|       Guy Himuro|  PU_CLERK|  2600|\n",
      "| Karen Colmenares|  PU_CLERK|  2500|\n",
      "+-----------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "sqlQry = spark.sql(\"SELECT CONCAT(first_name,' ',last_name) as FullName, job_id, salary FROM dataEmployees\")\n",
    "sqlQry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------+\n",
      "|         FullName|    job_id|salary|\n",
      "+-----------------+----------+------+\n",
      "|      Steven King|   AD_PRES| 24000|\n",
      "|    Neena Kochhar|     AD_VP| 17000|\n",
      "|      Lex De Haan|     AD_VP| 17000|\n",
      "| Alexander Hunold|   IT_PROG|  9000|\n",
      "|      Bruce Ernst|   IT_PROG|  6000|\n",
      "|     David Austin|   IT_PROG|  4800|\n",
      "|  Valli Pataballa|   IT_PROG|  4800|\n",
      "|    Diana Lorentz|   IT_PROG|  4200|\n",
      "|  Nancy Greenberg|    FI_MGR| 12008|\n",
      "|    Daniel Faviet|FI_ACCOUNT|  9000|\n",
      "|        John Chen|FI_ACCOUNT|  8200|\n",
      "|   Ismael Sciarra|FI_ACCOUNT|  7700|\n",
      "|Jose Manuel Urman|FI_ACCOUNT|  7800|\n",
      "|        Luis Popp|FI_ACCOUNT|  6900|\n",
      "|     Den Raphaely|    PU_MAN| 11000|\n",
      "|   Alexander Khoo|  PU_CLERK|  3100|\n",
      "|     Shelli Baida|  PU_CLERK|  2900|\n",
      "|     Sigal Tobias|  PU_CLERK|  2800|\n",
      "|       Guy Himuro|  PU_CLERK|  2600|\n",
      "| Karen Colmenares|  PU_CLERK|  2500|\n",
      "+-----------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Put equivalence Dataframe API script in here\n",
    "dfEmployees.select(concat(\"first_name\",lit(\" \"), \"last_name\").alias(\"FullName\"),\n",
    "                   \"job_id\",\n",
    "                   \"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Display employee's first name with their last name, along with their email address with the domain name (@monash.edu).\n",
    "Ensure you have the same format as expected output below\n",
    "![result](lab2_project1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|         FullName|              email|\n",
      "+-----------------+-------------------+\n",
      "|      Steven King|   sking@monash.edu|\n",
      "|    Neena Kochhar|nkochhar@monash.edu|\n",
      "|      Lex De Haan| ldehaan@monash.edu|\n",
      "| Alexander Hunold| ahunold@monash.edu|\n",
      "|      Bruce Ernst|  bernst@monash.edu|\n",
      "|     David Austin| daustin@monash.edu|\n",
      "|  Valli Pataballa|vpatabal@monash.edu|\n",
      "|    Diana Lorentz|dlorentz@monash.edu|\n",
      "|  Nancy Greenberg|ngreenbe@monash.edu|\n",
      "|    Daniel Faviet| dfaviet@monash.edu|\n",
      "|        John Chen|   jchen@monash.edu|\n",
      "|   Ismael Sciarra|isciarra@monash.edu|\n",
      "|Jose Manuel Urman| jmurman@monash.edu|\n",
      "|        Luis Popp|   lpopp@monash.edu|\n",
      "|     Den Raphaely|drapheal@monash.edu|\n",
      "|   Alexander Khoo|   akhoo@monash.edu|\n",
      "|     Shelli Baida|  sbaida@monash.edu|\n",
      "|     Sigal Tobias| stobias@monash.edu|\n",
      "|       Guy Himuro| ghimuro@monash.edu|\n",
      "| Karen Colmenares|kcolmena@monash.edu|\n",
      "+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "sqlQry = spark.sql(\"SELECT CONCAT(first_name,' ',last_name) as FullName, CONCAT(LOWER(email),'@monash.edu') as email FROM dataEmployees\")\n",
    "sqlQry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------------------------+\n",
      "|         FullName|concat(email, @monash.edu AS `email`)|\n",
      "+-----------------+-------------------------------------+\n",
      "|      Steven King|                     SKING@monash.edu|\n",
      "|    Neena Kochhar|                  NKOCHHAR@monash.edu|\n",
      "|      Lex De Haan|                   LDEHAAN@monash.edu|\n",
      "| Alexander Hunold|                   AHUNOLD@monash.edu|\n",
      "|      Bruce Ernst|                    BERNST@monash.edu|\n",
      "|     David Austin|                   DAUSTIN@monash.edu|\n",
      "|  Valli Pataballa|                  VPATABAL@monash.edu|\n",
      "|    Diana Lorentz|                  DLORENTZ@monash.edu|\n",
      "|  Nancy Greenberg|                  NGREENBE@monash.edu|\n",
      "|    Daniel Faviet|                   DFAVIET@monash.edu|\n",
      "|        John Chen|                     JCHEN@monash.edu|\n",
      "|   Ismael Sciarra|                  ISCIARRA@monash.edu|\n",
      "|Jose Manuel Urman|                   JMURMAN@monash.edu|\n",
      "|        Luis Popp|                     LPOPP@monash.edu|\n",
      "|     Den Raphaely|                  DRAPHEAL@monash.edu|\n",
      "|   Alexander Khoo|                     AKHOO@monash.edu|\n",
      "|     Shelli Baida|                    SBAIDA@monash.edu|\n",
      "|     Sigal Tobias|                   STOBIAS@monash.edu|\n",
      "|       Guy Himuro|                   GHIMURO@monash.edu|\n",
      "| Karen Colmenares|                  KCOLMENA@monash.edu|\n",
      "+-----------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Put equivalence Dataframe API script in here\n",
    "dfEmployees.select(concat(\"first_name\",lit(\" \"), \"last_name\").alias(\"FullName\"),\n",
    "                   concat(\"email\",lit(\"@monash.edu\").alias(\"email\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECTION QUERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Display the employees who work as *IT_PROG*\n",
    "\n",
    "![figure](lab2_selection0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|first_name|last_name|department_id|\n",
      "+----------+---------+-------------+\n",
      "| Alexander|   Hunold|           60|\n",
      "|     Bruce|    Ernst|           60|\n",
      "|     David|   Austin|           60|\n",
      "|     Valli|Pataballa|           60|\n",
      "|     Diana|  Lorentz|           60|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "sqlQry = spark.sql(\"SELECT first_name,last_name,department_id FROM dataEmployees WHERE job_id=='IT_PROG'\")\n",
    "sqlQry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|first_name|last_name|department_id|\n",
      "+----------+---------+-------------+\n",
      "| Alexander|   Hunold|           60|\n",
      "|     Bruce|    Ernst|           60|\n",
      "|     David|   Austin|           60|\n",
      "|     Valli|Pataballa|           60|\n",
      "|     Diana|  Lorentz|           60|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Put equivalence Dataframe API script in here\n",
    "dfEmployees.select(\"first_name\",\n",
    "                   \"last_name\",\n",
    "                   \"department_id\").where(\"job_id=='IT_PROG'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Display all employees that have the salary more than 5000 and work as \"Sales\" (job_id beginning with 'SA')\n",
    "\n",
    "![figure](lab2_selection1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|last_name|job_id|\n",
      "+---------+------+\n",
      "|  Russell|SA_MAN|\n",
      "| Partners|SA_MAN|\n",
      "|Errazuriz|SA_MAN|\n",
      "|Cambrault|SA_MAN|\n",
      "|  Zlotkey|SA_MAN|\n",
      "|   Tucker|SA_REP|\n",
      "|Bernstein|SA_REP|\n",
      "|     Hall|SA_REP|\n",
      "|    Olsen|SA_REP|\n",
      "|Cambrault|SA_REP|\n",
      "|  Tuvault|SA_REP|\n",
      "|     King|SA_REP|\n",
      "|    Sully|SA_REP|\n",
      "|   McEwen|SA_REP|\n",
      "|    Smith|SA_REP|\n",
      "|    Doran|SA_REP|\n",
      "|   Sewall|SA_REP|\n",
      "|  Vishney|SA_REP|\n",
      "|   Greene|SA_REP|\n",
      "|  Marvins|SA_REP|\n",
      "+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql\n",
    "sqlQry = spark.sql(\"SELECT last_name,job_id FROM dataEmployees WHERE salary>5000 AND SUBSTRING_INDEX(job_id,'_',1)=='SA'\")\n",
    "#search job_id by index\n",
    "\n",
    "#sqlQry = spark.sql(\"SELECT last_name,job_id FROM dataEmployees WHERE salary>5000 AND job_id like 'SA%'\")\n",
    "#search job_id by starting word\n",
    "sqlQry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|last_name|job_id|\n",
      "+---------+------+\n",
      "|  Russell|SA_MAN|\n",
      "| Partners|SA_MAN|\n",
      "|Errazuriz|SA_MAN|\n",
      "|Cambrault|SA_MAN|\n",
      "|  Zlotkey|SA_MAN|\n",
      "|   Tucker|SA_REP|\n",
      "|Bernstein|SA_REP|\n",
      "|     Hall|SA_REP|\n",
      "|    Olsen|SA_REP|\n",
      "|Cambrault|SA_REP|\n",
      "|  Tuvault|SA_REP|\n",
      "|     King|SA_REP|\n",
      "|    Sully|SA_REP|\n",
      "|   McEwen|SA_REP|\n",
      "|    Smith|SA_REP|\n",
      "|    Doran|SA_REP|\n",
      "|   Sewall|SA_REP|\n",
      "|  Vishney|SA_REP|\n",
      "|   Greene|SA_REP|\n",
      "|  Marvins|SA_REP|\n",
      "+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Put equivalence Dataframe API script in here\n",
    "dfEmployees.select(\"last_name\",\n",
    "                   \"job_id\").where(\"5000<salary\" and \"SUBSTRING_INDEX(job_id,'_',1)=='SA'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Find all employees that work in either department 50 or 60 that get paid for less than 5000\n",
    "\n",
    "![figure](lab2_selection2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+\n",
      "|first_name|  job_id|department_id|salary|\n",
      "+----------+--------+-------------+------+\n",
      "|     David| IT_PROG|           60|  4800|\n",
      "|     Valli| IT_PROG|           60|  4800|\n",
      "|     Diana| IT_PROG|           60|  4200|\n",
      "|     Julia|ST_CLERK|           50|  3200|\n",
      "|     Irene|ST_CLERK|           50|  2700|\n",
      "|     James|ST_CLERK|           50|  2400|\n",
      "|    Steven|ST_CLERK|           50|  2200|\n",
      "|     Laura|ST_CLERK|           50|  3300|\n",
      "|     Mozhe|ST_CLERK|           50|  2800|\n",
      "|     James|ST_CLERK|           50|  2500|\n",
      "|        TJ|ST_CLERK|           50|  2100|\n",
      "|     Jason|ST_CLERK|           50|  3300|\n",
      "|   Michael|ST_CLERK|           50|  2900|\n",
      "|        Ki|ST_CLERK|           50|  2400|\n",
      "|     Hazel|ST_CLERK|           50|  2200|\n",
      "|    Renske|ST_CLERK|           50|  3600|\n",
      "|   Stephen|ST_CLERK|           50|  3200|\n",
      "|      John|ST_CLERK|           50|  2700|\n",
      "|    Joshua|ST_CLERK|           50|  2500|\n",
      "|    Trenna|ST_CLERK|           50|  3500|\n",
      "+----------+--------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "sqlQry = spark.sql(\"SELECT first_name,job_id,department_id,salary FROM dataEmployees \"+\n",
    "                   \"WHERE (department_id=='50' OR department_id=='60') AND salary<5000\")\n",
    "sqlQry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+\n",
      "|first_name|  job_id|department_id|salary|\n",
      "+----------+--------+-------------+------+\n",
      "| Alexander| IT_PROG|           60|  9000|\n",
      "|     Bruce| IT_PROG|           60|  6000|\n",
      "|     David| IT_PROG|           60|  4800|\n",
      "|     Valli| IT_PROG|           60|  4800|\n",
      "|     Diana| IT_PROG|           60|  4200|\n",
      "|   Matthew|  ST_MAN|           50|  8000|\n",
      "|      Adam|  ST_MAN|           50|  8200|\n",
      "|     Payam|  ST_MAN|           50|  7900|\n",
      "|    Shanta|  ST_MAN|           50|  6500|\n",
      "|     Kevin|  ST_MAN|           50|  5800|\n",
      "|     Julia|ST_CLERK|           50|  3200|\n",
      "|     Irene|ST_CLERK|           50|  2700|\n",
      "|     James|ST_CLERK|           50|  2400|\n",
      "|    Steven|ST_CLERK|           50|  2200|\n",
      "|     Laura|ST_CLERK|           50|  3300|\n",
      "|     Mozhe|ST_CLERK|           50|  2800|\n",
      "|     James|ST_CLERK|           50|  2500|\n",
      "|        TJ|ST_CLERK|           50|  2100|\n",
      "|     Jason|ST_CLERK|           50|  3300|\n",
      "|   Michael|ST_CLERK|           50|  2900|\n",
      "+----------+--------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Put equivalence Dataframe API script in here\n",
    "lstQry=dfEmployees.select(\"first_name\",\n",
    "                   \"job_id\",\n",
    "                   \"department_id\",\n",
    "                   \"salary\").where(\"salary<5000\" and \"department_id==60 OR department_id==50\")\n",
    "lstQry.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPORTING RESULT TO EXTERNAL FILES\n",
    "\n",
    "#### Question 6\n",
    "Save the last query (Question 5) to csv file. Named the output folder as 'result' \n",
    "\n",
    "Make sure result HEADER is written to the file and file mode is OVERWRITE. \n",
    "\n",
    "Check your folder to verify the output file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstQry.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
