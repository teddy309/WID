{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name : < Put your name in this section >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access using SparkSQL and Dataframe\n",
    "\n",
    "## Activity : Aggregation and Sorting Queries\n",
    "\n",
    "In this module, you will practice how to write codes to retrieve data using Spark SQL and Dataframes API.\n",
    "\n",
    "The complete list of Dataframe functions can be accessed from [here](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html), [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) and [here](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.functions$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we will use HR schema as shown below\n",
    "![hr](HR.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION\n",
    "The first section of this scipt is the initialization section. \n",
    "In this section, we are preparing Spark environment to recognize and process SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#additional \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "# In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). \n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as `local[k]`.\n",
    "# The `appName` field is a name to be shown on the Sparking cluster UI. \n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[3]\", appName=\"Week 2 - Join Query\")\n",
    "spark = SparkSession(sparkContext=sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA STRUCTURE DEFINITION\n",
    "\n",
    "In this section, we are preparing the data structure to match the datafiles provided as the datasources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES TABLE\n",
    "scCountries = StructType([StructField(\"country_id\",StringType()),StructField(\"country_name\",StringType()),StructField(\"region_id\",IntegerType())])\n",
    "\n",
    "#DEPARTMENTS TABLE\n",
    "scDepartments = StructType([StructField(\"department_id\",IntegerType()),\n",
    "StructField(\"department_name\",StringType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"location_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#EMPLOYEES TABLE\n",
    "scEmployees = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"first_name\",StringType()),\n",
    "StructField(\"last_name\",StringType()),\n",
    "StructField(\"email\",StringType()),\n",
    "StructField(\"phone_number\",StringType()),\n",
    "StructField(\"hire_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"salary\",IntegerType()),\n",
    "StructField(\"commission_pct\",FloatType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOBS TABLE\n",
    "scJobs = StructType([\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"job_title\",StringType()),\n",
    "StructField(\"min_salary\",IntegerType()),\n",
    "StructField(\"max_salary\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOB_HISTORY TABLE\n",
    "scJob_history = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"start_date\",StringType()),\n",
    "StructField(\"end_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#LOCATIONS TABLE\n",
    "scLocations = StructType([\n",
    "StructField(\"location_id\",IntegerType()),\n",
    "StructField(\"street_address\",StringType()),\n",
    "StructField(\"postal_code\",StringType()),\n",
    "StructField(\"city\",StringType()),\n",
    "StructField(\"state_province\",StringType()),\n",
    "StructField(\"country_id\",StringType())\n",
    "])\n",
    "\n",
    "#REGIONS TABLE\n",
    "scRegions = StructType([\n",
    "StructField(\"region_id\",IntegerType()),\n",
    "StructField(\"region_name\",StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES DATA\n",
    "dataCountries = sc.textFile('COUNTRIES.csv')\n",
    "dataCountries = dataCountries.map(lambda x: x.split(','))\n",
    "dataCountries = dataCountries.map(lambda x: [x[0],x[1], int(x[2])])\n",
    "\n",
    "#DEPARTMENTS DATA\n",
    "dataDepartments = sc.textFile('DEPARTMENTS.csv')\n",
    "dataDepartments = dataDepartments.map(lambda x: x.split(','))\n",
    "dataDepartments = dataDepartments.map(lambda x: [int(x[0]),x[1], int(x[2]), int(x[3])])\n",
    "\n",
    "#EMPLOYEES DATA\n",
    "dataEmployees = sc.textFile('EMPLOYEES.csv')\n",
    "dataEmployees = dataEmployees.map(lambda x: x.split(','))\n",
    "dataEmployees = dataEmployees.map(lambda x: [int(x[0]),x[1], x[2], \\\n",
    "                                             x[3],x[4], x[5], x[6], \\\n",
    "                                             int(x[7]),float(x[8]), int(x[9]), int(x[10])\\\n",
    "                                            ])\n",
    "\n",
    "#JOBS_DATA\n",
    "dataJobs = sc.textFile('JOBS.csv')\n",
    "dataJobs = dataJobs.map(lambda x: x.split(','))\n",
    "dataJobs = dataJobs.map(lambda x: [x[0],x[1], \\\n",
    "                                   int(x[2]),int(x[3])\\\n",
    "                                   ])\n",
    "\n",
    "#JOB_HISTORY_DATA\n",
    "dataJob_history = sc.textFile('JOB_HISTORY.csv')\n",
    "dataJob_history = dataJob_history.map(lambda x: x.split(','))\n",
    "dataJob_history = dataJob_history.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],int(x[4])\\\n",
    "                                   ])\n",
    "\n",
    "#LOCATION_DATA\n",
    "dataLocations = sc.textFile('LOCATIONS.csv')\n",
    "dataLocations = dataLocations.map(lambda x: x.split(','))\n",
    "dataLocations = dataLocations.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],x[4],x[5]\\\n",
    "                                   ])\n",
    "#REGIONS DATA\n",
    "dataRegions = sc.textFile('REGIONS.csv')\n",
    "dataRegions = dataRegions.map(lambda x: x.split(','))\n",
    "dataRegions = dataRegions.map(lambda x: [int(x[0]),x[1] ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCountries = spark.createDataFrame(dataCountries,schema=scCountries) \n",
    "dfCountries.createOrReplaceTempView(\"dataCountries\")\n",
    "\n",
    "dfDepartments = spark.createDataFrame(dataDepartments,schema=scDepartments) \n",
    "dfDepartments.createOrReplaceTempView(\"dataDepartments\")\n",
    "\n",
    "dfEmployees = spark.createDataFrame(dataEmployees,schema=scEmployees) \n",
    "dfEmployees.createOrReplaceTempView(\"dataEmployees\")\n",
    "\n",
    "dfJobs = spark.createDataFrame(dataJobs,schema=scJobs) \n",
    "dfJobs.createOrReplaceTempView(\"dataJobs\")\n",
    "\n",
    "dfJob_history = spark.createDataFrame(dataJob_history,schema=scJob_history) \n",
    "dfJob_history.createOrReplaceTempView(\"dataJob_history\")\n",
    "\n",
    "dfLocations = spark.createDataFrame(dataLocations,schema=scLocations) \n",
    "dfLocations.createOrReplaceTempView(\"dataLocations\")\n",
    "\n",
    "dfRegions = spark.createDataFrame(dataRegions,schema=scRegions) \n",
    "dfRegions.createOrReplaceTempView(\"dataRegions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Display all department name and the number of employees. Sort the result according to department name in descending order.\n",
    "\n",
    "Ensure you have the same format as expected output below \n",
    "\n",
    "![picture](lab4_q1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|     department_name|total|\n",
      "+--------------------+-----+\n",
      "|       Corporate Tax|    0|\n",
      "|               Sales|   34|\n",
      "|          Accounting|    2|\n",
      "|    Government Sales|    0|\n",
      "|             Payroll|    0|\n",
      "|             Finance|    6|\n",
      "|    Public Relations|    1|\n",
      "|           Executive|    3|\n",
      "|          Recruiting|    0|\n",
      "|          Purchasing|    6|\n",
      "|        Construction|    0|\n",
      "|                 NOC|    0|\n",
      "|            Treasury|    0|\n",
      "|Shareholder Services|    0|\n",
      "|        Retail Sales|    0|\n",
      "|           Marketing|    2|\n",
      "|                  IT|    5|\n",
      "|      Administration|    1|\n",
      "|         Contracting|    0|\n",
      "|          IT Support|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "sqlQry=spark.sql(\"SELECT department_name, COUNT(dataEmployees.department_id=dataDepartments.department_id) as total \"+\n",
    "          \"FROM dataDepartments LEFT JOIN dataEmployees \" +\n",
    "          \"ON dataEmployees.department_id=dataDepartments.department_id \" +\n",
    "          \"GROUP BY department_name\")\n",
    "sqlQry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|     department_name|total|\n",
      "+--------------------+-----+\n",
      "|       Corporate Tax|    1|\n",
      "|               Sales|   34|\n",
      "|          Accounting|    2|\n",
      "|    Government Sales|    1|\n",
      "|             Payroll|    1|\n",
      "|             Finance|    6|\n",
      "|    Public Relations|    1|\n",
      "|           Executive|    3|\n",
      "|          Recruiting|    1|\n",
      "|          Purchasing|    6|\n",
      "|        Construction|    1|\n",
      "|                 NOC|    1|\n",
      "|            Treasury|    1|\n",
      "|Shareholder Services|    1|\n",
      "|        Retail Sales|    1|\n",
      "|           Marketing|    2|\n",
      "|                  IT|    5|\n",
      "|      Administration|    1|\n",
      "|         Contracting|    1|\n",
      "|          IT Support|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame functions\n",
    "q1_1 = dfDepartments.join(dfEmployees, dfDepartments.department_id==dfEmployees.department_id, 'left')\n",
    "q1_2 = q1_1.groupBy('department_name').count()\n",
    "q1_3 = q1_2.select(\"department_name\", \"count\").withColumnRenamed('count', 'total').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Using the result from Question 1, display the departments that have more than 20 employees. \n",
    "Order the result according to the number of employees in descending order.\n",
    "\n",
    "![figure](lab4_q2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|department_name|total|\n",
      "+---------------+-----+\n",
      "|       Shipping|   45|\n",
      "|          Sales|   34|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql\n",
    "q2=spark.sql(\"select department_name, count(employee_id) as total\" +\n",
    "          \" from dataEmployees as de INNER JOIN dataDepartments as dd on de.department_id=dd.department_id\" +\n",
    "          \" group by department_name having count(*) > 20 order by department_name DESC\")\n",
    "q2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|department_name|total|\n",
      "+---------------+-----+\n",
      "|       Shipping|   45|\n",
      "|          Sales|   34|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe function\n",
    "dfEmployees.join(dfDepartments, dfEmployees.department_id == dfDepartments.department_id,\\\n",
    "                ).groupBy(\"department_name\").agg(count(\"employee_id\").alias(\"total\")).filter(count(\"employee_id\") > 20).orderBy(\"department_name\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Find the employee name and salary that gets the lowest salary in the company.\n",
    "\n",
    "![figure](lab4_q3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|first_name|salary|\n",
      "+----------+------+\n",
      "|        TJ|  2100|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "q3 = spark.sql(\"select first_name, salary from dataEmployees order by salary LIMIT 1\")\n",
    "q3.show(1)#select lowest salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|first_name|salary|\n",
      "+----------+------+\n",
      "|        TJ|  2100|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe functions\n",
    "dfEmployees.select(\"first_name\", \"salary\").orderBy(\"salary\").limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "Find the first employee that join in the company. Display his name, job and his hire date.\n",
    "\n",
    "![figure](lab4_q4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+---------+\n",
      "|first_name|last_name|job_id|  recruit|\n",
      "+----------+---------+------+---------+\n",
      "|       Lex|  De Haan| AD_VP|13-Jan-01|\n",
      "+----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql\n",
    "hiredate = spark.sql(\"select first_name, last_name, job_id, hire_date as recruit \" +\n",
    "                 \"from dataEmployees order by reverse(hire_date)\").limit(1)\n",
    "hiredate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+---------+\n",
      "|first_name|last_name|job_id|  recruit|\n",
      "+----------+---------+------+---------+\n",
      "|       Lex|  De Haan| AD_VP|13-Jan-01|\n",
      "+----------+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe\n",
    "dfEmployees.select(\"first_name\", \"last_name\", \"job_id\", \"hire_Date\",\\\n",
    "                  ).withColumnRenamed(\"hire_date\", \"recruit\").orderBy(reverse(\"hire_date\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Display department name that has the highest total salary in the company.\n",
    "\n",
    "![figure](lab4_q5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|department_name| total|\n",
      "+---------------+------+\n",
      "|          Sales|304500|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "highSalary=spark.sql(\"select department_name, sum(salary) as total from dataDepartments LEFT JOIN dataEmployees \"+\n",
    "                     \"ON dataEmployees.department_id=dataDepartments.department_id \"+\n",
    "                     \"group by department_name order by sum(salary) DESC LIMIT 1\")\n",
    "highSalary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|department_name| total|\n",
      "+---------------+------+\n",
      "|          Sales|304500|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe function\n",
    "q5_1 = dfDepartments.join(dfEmployees, dfDepartments.department_id==dfEmployees.department_id, 'left')\n",
    "q5_2 = q5_1.groupBy('department_name').sum('salary')\n",
    "q5_2.select(\"department_name\", \"sum(salary)\").withColumnRenamed('sum(salary)', 'total').orderBy(desc(\"sum(salary)\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
