{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access using SparkSQL and Dataframe\n",
    "\n",
    "## Activity : Projection and Selection Queries\n",
    "\n",
    "In this module, you will practice how to write codes to retrieve data using Spark SQL and Dataframes API.\n",
    "\n",
    "The complete list of Dataframe functions can be accessed from [here](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html), [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) and [here](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.functions$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we will use HR schema as shown below\n",
    "![hr](HR.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION\n",
    "The first section of this scipt is the initialization section. \n",
    "In this section, we are preparing Spark environment to recognize and process SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#additional \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "# In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). \n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as `local[k]`.\n",
    "# The `appName` field is a name to be shown on the Sparking cluster UI. \n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[3]\", appName=\"Introduction to Apache Spark\")\n",
    "spark = SparkSession(sparkContext=sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA STRUCTURE DEFINITION\n",
    "\n",
    "In this section, we are preparing the data structure to match the datafiles provided as the datasources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES TABLE\n",
    "scCountries = StructType([StructField(\"country_id\",StringType()),StructField(\"country_name\",StringType()),StructField(\"region_id\",IntegerType())])\n",
    "\n",
    "#DEPARTMENTS TABLE\n",
    "scDepartments = StructType([StructField(\"department_id\",IntegerType()),\n",
    "StructField(\"department_name\",StringType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"location_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#EMPLOYEES TABLE\n",
    "scEmployees = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"first_name\",StringType()),\n",
    "StructField(\"last_name\",StringType()),\n",
    "StructField(\"email\",StringType()),\n",
    "StructField(\"phone_number\",StringType()),\n",
    "StructField(\"hire_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"salary\",IntegerType()),\n",
    "StructField(\"commission_pct\",FloatType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOBS TABLE\n",
    "scJobs = StructType([\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"job_title\",StringType()),\n",
    "StructField(\"min_salary\",IntegerType()),\n",
    "StructField(\"max_salary\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOB_HISTORY TABLE\n",
    "scJob_history = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"start_date\",StringType()),\n",
    "StructField(\"end_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#LOCATIONS TABLE\n",
    "scLocations = StructType([\n",
    "StructField(\"location_id\",IntegerType()),\n",
    "StructField(\"street_address\",StringType()),\n",
    "StructField(\"postal_code\",StringType()),\n",
    "StructField(\"city\",StringType()),\n",
    "StructField(\"state_province\",StringType()),\n",
    "StructField(\"country_id\",StringType())\n",
    "])\n",
    "\n",
    "#REGIONS TABLE\n",
    "scRegions = StructType([\n",
    "StructField(\"region_id\",IntegerType()),\n",
    "StructField(\"region_name\",StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES DATA\n",
    "dataCountries = sc.textFile('COUNTRIES.csv')\n",
    "dataCountries = dataCountries.map(lambda x: x.split(','))\n",
    "dataCountries = dataCountries.map(lambda x: [x[0],x[1], int(x[2])])\n",
    "\n",
    "#DEPARTMENTS DATA\n",
    "dataDepartments = sc.textFile('DEPARTMENTS.csv')\n",
    "dataDepartments = dataDepartments.map(lambda x: x.split(','))\n",
    "dataDepartments = dataDepartments.map(lambda x: [int(x[0]),x[1], int(x[2]), int(x[3])])\n",
    "\n",
    "#EMPLOYEES DATA\n",
    "dataEmployees = sc.textFile('EMPLOYEES.csv')\n",
    "dataEmployees = dataEmployees.map(lambda x: x.split(','))\n",
    "dataEmployees = dataEmployees.map(lambda x: [int(x[0]),x[1], x[2], \\\n",
    "                                             x[3],x[4], x[5], x[6], \\\n",
    "                                             int(x[7]),float(x[8]), int(x[9]), int(x[10])\\\n",
    "                                            ])\n",
    "\n",
    "#JOBS_DATA\n",
    "dataJobs = sc.textFile('JOBS.csv')\n",
    "dataJobs = dataJobs.map(lambda x: x.split(','))\n",
    "dataJobs = dataJobs.map(lambda x: [x[0],x[1], \\\n",
    "                                   int(x[2]),int(x[3])\\\n",
    "                                   ])\n",
    "\n",
    "#JOB_HISTORY_DATA\n",
    "dataJob_history = sc.textFile('JOB_HISTORY.csv')\n",
    "dataJob_history = dataJob_history.map(lambda x: x.split(','))\n",
    "dataJob_history = dataJob_history.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],int(x[4])\\\n",
    "                                   ])\n",
    "\n",
    "#LOCATION_DATA\n",
    "dataLocations = sc.textFile('LOCATIONS.csv')\n",
    "dataLocations = dataLocations.map(lambda x: x.split(','))\n",
    "dataLocations = dataLocations.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],x[4],x[5]\\\n",
    "                                   ])\n",
    "#REGIONS DATA\n",
    "dataRegions = sc.textFile('REGIONS.csv')\n",
    "dataRegions = dataRegions.map(lambda x: x.split(','))\n",
    "dataRegions = dataRegions.map(lambda x: [int(x[0]),x[1] ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCountries = spark.createDataFrame(dataCountries,schema=scCountries) \n",
    "dfCountries.createOrReplaceTempView(\"dataCountries\")\n",
    "\n",
    "dfDepartments = spark.createDataFrame(dataDepartments,schema=scDepartments) \n",
    "dfDepartments.createOrReplaceTempView(\"dataDepartments\")\n",
    "\n",
    "dfEmployees = spark.createDataFrame(dataEmployees,schema=scEmployees) \n",
    "dfEmployees.createOrReplaceTempView(\"dataEmployees\")\n",
    "\n",
    "dfJobs = spark.createDataFrame(dataJobs,schema=scJobs) \n",
    "dfJobs.createOrReplaceTempView(\"dataJobs\")\n",
    "\n",
    "dfJob_history = spark.createDataFrame(dataJob_history,schema=scJob_history) \n",
    "dfJob_history.createOrReplaceTempView(\"dataJob_history\")\n",
    "\n",
    "dfLocations = spark.createDataFrame(dataLocations,schema=scLocations) \n",
    "dfLocations.createOrReplaceTempView(\"dataLocations\")\n",
    "\n",
    "dfRegions = spark.createDataFrame(dataRegions,schema=scRegions) \n",
    "dfRegions.createOrReplaceTempView(\"dataRegions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# kafka consumer connect\n",
    "consumer = KafkaConsumer(\n",
    "    'dfTest',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my_group',\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diana Lorentz\n",
      "Nancy Greenberg\n",
      "Daniel Faviet\n",
      "John Chen\n",
      "Ismael Sciarra\n",
      "Jose Manuel Urman\n",
      "Luis Popp\n",
      "Den Raphaely\n",
      "Alexander Khoo\n",
      "Shelli Baida\n",
      "Sigal Tobias\n",
      "Guy Himuro\n",
      "Karen Colmenares\n",
      "Matthew Weiss\n",
      "Adam Fripp\n",
      "Payam Kaufling\n",
      "Shanta Vollman\n",
      "Kevin Mourgos\n",
      "Julia Nayer\n",
      "Irene Mikkilineni\n",
      "James Landry\n",
      "Steven Markle\n",
      "Laura Bissot\n",
      "Mozhe Atkinson\n",
      "James Marlow\n",
      "TJ Olson\n",
      "Jason Mallin\n",
      "Michael Rogers\n",
      "Ki Gee\n",
      "Hazel Philtanker\n",
      "Renske Ladwig\n",
      "Stephen Stiles\n",
      "John Seo\n",
      "Joshua Patel\n",
      "Trenna Rajs\n",
      "Curtis Davies\n",
      "Randall Matos\n",
      "Peter Vargas\n",
      "John Russell\n",
      "Karen Partners\n",
      "Alberto Errazuriz\n",
      "Gerald Cambrault\n",
      "Eleni Zlotkey\n",
      "Peter Tucker\n",
      "David Bernstein\n",
      "Peter Hall\n",
      "Christopher Olsen\n",
      "Nanette Cambrault\n",
      "Steven King\n",
      "Neena Kochhar\n",
      "Lex De Haan\n",
      "Alexander Hunold\n",
      "Bruce Ernst\n",
      "David Austin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6530c46e475d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'row'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/FIT5202/jupyter/lib/python3.6/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_consumer_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/FIT5202/jupyter/lib/python3.6/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_message_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_flight_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m                 \u001b[0mpoll_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;31m# We need to make sure we at least keep up with scheduled tasks,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/FIT5202/jupyter/lib/python3.6/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, future, delayed_tasks)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# avoid negative timeouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;31m# If all we had was a timeout (future is None) - only do one poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/FIT5202/jupyter/lib/python3.6/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mstart_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mend_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for message in consumer:\n",
    "    message = message.value\n",
    "    print(message['row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
